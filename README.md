假设函数Ｊ(θ)

代价函数ｈ<sub>θ</sub>(x)



#### 梯度下降函数(Gradient Descent)

当特征值的取值范围较大或较小时(绝对值大于3或者小于0.3),需要进行特征缩放.

##### 目的

加快梯度下降的速度

##### 具体操作

令所有的特征值x减去x的平均值除以范围的差值即可,如范围是0~2000,那么每个特征值都改为(x-1000/2000),那么范围就是-0.5<=x<=0.5.



#### 代价函数

寻找最小代价函数的两种方法

##### 梯度下降

###### 方式

不停的进行迭代,寻找特定的θ值令代价函数J(θ)为最小值

###### 优点
当特征数n大时,迭代的速度仍然很快

###### 缺点
需要选择学习效率α
需要进行多次迭代



##### 正规方程

方式:先分析得到设计矩阵(design matrix)**X**,再经过运算得到θ的向量.

###### 求设计矩阵**X**

X=

| (X<sup>(1)</sup>)<sup>T</sup> |
| :---------------------------: |
| (X<sup>(2)</sup>)<sup>T</sup> |
|              ...              |
| (X<sup>(m)</sup>)<sup>T</sup> |

其中,m表示训练集的总数,X<sup>1</sup>表示第一行各个特征值对应的数据

###### θ运算公式

θ=**(X<sup>T</sup>X)<sup>-1</sup>X<sup>T</sup>y**

###### 优点

不需要选择学习效率α
不需要进行迭代

###### 缺点

当特征数n大时,运算非常慢,因为要计算矩阵(X<sup>T</sup>X)<sup>-1</sup>X<sup>T</sup>y的值,时间复杂度为O(n<sup>3</sup>)





